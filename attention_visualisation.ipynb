{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87348e94-b704-4ed9-95ce-1a1dd54bac54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertviz import head_view, model_view\n",
    "from transformers import AutoTokenizer, AutoModel, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d9b554-9c9c-4b96-9e83-1997a2ad5158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model load\n",
    "base_name = \"sentence-transformers/all-MiniLM-L12-v2\"  \n",
    "ft_name = \"/workspace/NER_for_ERC/train/output_models/re_minilm_v1/5600\" # trained model path\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_name)\n",
    "\n",
    "base_model = AutoModel.from_pretrained(base_name, output_attentions=True)  # Configure model to return attention values\n",
    "ft_model = AutoModel.from_pretrained(ft_name, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843d9a29-ff2e-49e4-b321-fd6340f4c38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Help! I forgot my PIN and have been locked out of using my card.\"  \n",
    "inputs = tokenizer.encode(input_text, return_tensors='pt')  # Tokenize input text\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[0])  #\n",
    "\n",
    "base_outputs = base_model(inputs)  # Run model\n",
    "base_attention = base_outputs[-1]  # Retrieve attention from model outputs\n",
    "\n",
    "ft_outputs = ft_model(inputs)\n",
    "ft_attention = ft_outputs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00168ce4-2648-4642-b72b-159f940620b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ‘‰ Hover over any token on the left/right side of the visualization to filter attention from/to that token.\n",
    "# ðŸ‘‰ Double-click on any of the colored tiles at the top to filter to the corresponding attention head.\n",
    "# ðŸ‘‰ Single-click on any of the colored tiles to toggle selection of the corresponding attention head.\n",
    "# ðŸ‘‰ Click on the Layer drop-down to change the model layer (zero-indexed).\n",
    "# https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1\n",
    "# https://colab.research.google.com/drive/1hXIQ77A4TYS4y3UthWF-Ci7V7vVUoxmQ?usp=sharing#scrollTo=YLAhBxDSScmV\n",
    "\n",
    "# ## Visualising attention of the last layer gives the greatest representation of high-level semantics. ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657f992d-ebea-4128-8f43-c9ad40a93755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base model head_view\n",
    "head_view(base_attention, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40efc961-8ded-479e-9e7e-9b243e9b743a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft model head_view\n",
    "head_view(ft_attention, tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
